<!doctype html><meta charset=utf-8><meta name=viewport content="width=device-width"><title>RWKV-v4 Web Demo</title><script src=github-pages-coop-coep-workaround.js></script><h1>RWKV-v4 Web Demo</h1><p>See <a href=https://github.com/josephrocca/rwkv-v4-web target=_blank>the Github repo</a> for more details about this demo. The inference speed numbers are just for my laptop - consider them as relative numbers at most. Obviously varies by device. Note that opening the browser console/DevTools currently slows down inference, even after you close it.<hr><div id=modelChoiceEl><div style=max-width:750px>Choose a model:</div><table id=modelTableEl><tr style=font-weight:700><td>Name<td>Params<td>File size<td>Inference speed<td>Notes<tr><td>rwkv-4-pile-169m.onnx<td>169m<td>679 MB<td>~32 tokens/sec<td>-<td><button onclick='modelUrlInputEl.value="https://huggingface.co/rocca/rwkv-4-pile-web/resolve/main/169m/rwkv-4-pile-169m.onnx",modelNumLayersEl.value="12",modelEmbedDimEl.value="768",backendEl.value="wasm",loadBtnEl.click()'>load</button><tr><td>rwkv-4-pile-169m-uint8.onnx<td>169m<td>171 MB<td>~12 tokens/sec<td>uint8 quantized - smaller but slower<td><button onclick='modelUrlInputEl.value="https://huggingface.co/rocca/rwkv-4-pile-web/resolve/main/169m/rwkv-4-pile-169m-uint8.onnx",modelNumLayersEl.value="12",modelEmbedDimEl.value="768",backendEl.value="wasm",loadBtnEl.click()'>load</button><tr><td>rwkv-4-pile-169m-webgl.onnx<td>169m<td>680 MB<td>~16 tokens/sec<td><a href=https://github.com/BlinkDL/RWKV-LM/issues/7#issuecomment-1225906768 target=_blank>webgl-compatible</a><td><button onclick='modelUrlInputEl.value="https://huggingface.co/rocca/rwkv-4-pile-web/resolve/main/169m/rwkv-4-pile-169m-webgl.onnx",modelNumLayersEl.value="12",modelEmbedDimEl.value="768",backendEl.value="webgl",loadBtnEl.click()'>load</button><tr style=opacity:.3><td>rwkv-4-pile-430m.onnx<td>430m<td>1.73 GB<td>?<td style=color:red>"RuntimeError: Aborted()" - too big to init<td><button onclick='modelUrlInputEl.value="https://huggingface.co/rocca/rwkv-4-pile-web/resolve/main/430m/rwkv-4-pile-430m.onnx",modelNumLayersEl.value="24",modelEmbedDimEl.value="1024",backendEl.value="wasm",loadBtnEl.click()'>load</button><tr><td>rwkv-4-pile-430m.with_runtime_opt.ort<td>430m<td>1.73 GB<td>~12 tokens/sec<td>ORT format<td><button onclick='modelUrlInputEl.value="https://huggingface.co/rocca/rwkv-4-pile-web/resolve/main/430m/rwkv-4-pile-430m.with_runtime_opt.ort",modelNumLayersEl.value="24",modelEmbedDimEl.value="1024",backendEl.value="wasm",loadBtnEl.click()'>load</button><tr><td>rwkv-4-pile-430m-uint8.onnx<td>430m<td>434 MB<td>~4 tokens/sec<td>uint8 quantized - smaller but slower<td><button onclick='modelUrlInputEl.value="https://huggingface.co/rocca/rwkv-4-pile-web/resolve/main/430m/rwkv-4-pile-430m-uint8.onnx",modelNumLayersEl.value="24",modelEmbedDimEl.value="1024",backendEl.value="wasm",loadBtnEl.click()'>load</button><tr><td>rwkv-4-pile-430m-webgl.onnx<td>430m<td>1.73 GB<td>~10 tokens/sec<td><a href=https://github.com/BlinkDL/RWKV-LM/issues/7#issuecomment-1225906768 target=_blank>webgl-compatible</a><td><button onclick='modelUrlInputEl.value="https://huggingface.co/rocca/rwkv-4-pile-web/resolve/main/430m/rwkv-4-pile-430m-webgl.onnx",modelNumLayersEl.value="24",modelEmbedDimEl.value="1024",backendEl.value="webgl",loadBtnEl.click()'>load</button></table><style>#modelTableEl{border-collapse:collapse;margin-top:1rem}#modelTableEl td,#modelTableEl th{border:1px solid grey}</style><div style=max-width:750px;margin-top:1rem>You can also load a custom model using a Hugging Face model URL. The URL must be a <u>direct</u> link to the model file. I.e. it must contain <b>/resolve/</b>, not /blob/ and must end in <i>.onnx</i> or <i>.ort</i></div><div style=background:#d3d3d3;padding:.5rem;width:fit-content><div><input id=modelUrlInputEl style=width:700px value=https://huggingface.co/rocca/rwkv-4-pile-web/resolve/main/169m/rwkv-4-pile-169m-uint8.onnx></div><div>Num Layers: <input id=modelNumLayersEl style=width:30px value=12></div><div>Embed Dim: <input id=modelEmbedDimEl style=width:30px value=768></div><div>Backend: <select id=backendEl><option value=wasm>wasm<option value=webgl>webgl (buggy)</select></div><button id=loadBtnEl onclick=resolveLoadClick()>load custom model</button></div></div><div id=loadingMessageEl style=display:none><div id=loadingStatusEl>Loading...</div><progress id=downloadProgressEl></progress></div><div id=inputAreaEl style=display:none><div id=modelInfoDisplayEl style=opacity:.5></div><select id=exampleSelectorEl onchange="promptInputEl.value=this.value" style=display:block><option value="The capital of Australia is Canberra.
The capital of France is Paris.
The capital of Peru is">country capitals<option value="7*3=21
21+2=23
1+2*4=9
7*2+31=">basic math<option value="QUESTION: The house is empty. Two people talk into the house. One person walks out, but then walks back in. A dog walks into the house. A person walks out of the house. How many people are in the house?
ANSWER: Let's think step by step. First,">word problem<option value="Jamie: Hey, did you hear the news?
Sam: No, what happened?
Jamie: They're finally trialling gene drives to combat malaria in Nigeria!
Sam:">chat<option value="dog --> fur --> soft --> teddy --> bear --> mammal --> taxonomy --> hierarchy --> king --> medieval -->">relationship chain</select> <textarea id=promptInputEl style=width:550px;height:250px></textarea><script>promptInputEl.value=exampleSelectorEl.querySelectorAll("option")[0].value</script><div>Num output tokens (including prompt): <input id=numTokensToGenInputEl type=number value=128 style=width:60px> <span id=progressDisplayEl></span> <span id=tokensPerSecDisplayEl style=opacity:.6></span></div><button id=generateButtonEl>generate text</button></div><script src=https://cdn.jsdelivr.net/pyodide/v0.21.0a2/full/pyodide.js></script><script src=https://cdn.jsdelivr.net/npm/onnxruntime-web@1.13.1/dist/ort.js></script><script type=module>await new Promise(r => window.resolveLoadClick=r); // wait for load button click
    
    let onnxModelUrl = modelUrlInputEl.value;
    let n_layer = Number(modelNumLayersEl.value);
    let n_embd = Number(modelEmbedDimEl.value);
		let backend = backendEl.value;
    
    modelInfoDisplayEl.innerHTML = `<b>url:</b>${onnxModelUrl} <b>n_layer:</b>${n_layer} <b>n_embd:</b>${n_embd} <b>backend:</b>${backend}`;
    
    modelChoiceEl.style.display = "none";
    loadingMessageEl.style.display = "";

    window.pyodide = await loadPyodide();
    await pyodide.loadPackage("micropip");

    // Add tokenizer.json file to Pyodide filesystem:
    let tokenizerJsonText = await fetch("./tokenizer.json").then(r => r.text())
    pyodide.FS.writeFile("/tokenizer.json", tokenizerJsonText, { encoding: "utf8" });

    // Install tokenizer:
    console.log(await pyodide.runPythonAsync(`
      import sys
      print(sys.version)
      import os
      os.environ["TOKENIZERS_PARALLELISM"] = "0" # This is needed because threading doesn't work in Pyodide yet: https://github.com/pyodide/pyodide/issues/2816#issue-1290719241
      import micropip
      await micropip.install('./tokenizers_python-0.11.0-cp310-cp310-emscripten_3_1_14_wasm32.whl')
      from tokenizers import Tokenizer
      tokenizer = Tokenizer.from_file("/tokenizer.json")
    `));
    
    function textToTokens(text) {
      pyodide.globals.set("input_text", text);
      pyodide.runPython(`
        encoded = tokenizer.encode(input_text)
        ids = encoded.ids
        tokens = encoded.tokens
      `);
      // pyodide.globals.get('tokens').toJs()
      return pyodide.globals.get('ids').toJs();
    }
    
    function tokensToText(tokens) {
      pyodide.globals.set("input_tokens", tokens.join(","));
      pyodide.runPython(`
        input_tokens = [int(x) for x in input_tokens.split(',')]
        decoded = tokenizer.decode(input_tokens)
      `);
      return pyodide.globals.get('decoded');
    }
    
    function padLeftWithZeros(arr, size) {
      arr = arr.slice(0);

      while (arr.length < size) {
        arr.unshift(0);
      }

      return arr;
    }

    function greedySampling(x) {
      let max_k = 0;
      let max_v = x[0];

      for (let i = 1; i < 50277; i++) {
        if (x[i] > max_v) {
          max_v = x[i];
          max_k = i;
        }
      }

      return max_k;
    }


    function downloadBlobWithProgress(url, onProgress) {
      return new Promise((res, rej) => {
        var blob;
        var xhr = new XMLHttpRequest();
        xhr.open('GET', url, true);
        xhr.responseType = 'arraybuffer';
        xhr.onload = function(e) {
          blob = new Blob([this.response]);   
        };
        xhr.onprogress = onProgress;
        xhr.onloadend = function(e){
          res(blob);
        }
        xhr.send();
      });
    }


    let session;
    try {
      ort.env.wasm.proxy = true; // <-- When using wasm, proxy inference via a web worker so it doesn't freeze the main/rendering thread.
      
      if(self.crossOriginIsolated) { // needs to be cross-origin-isolated to use wasm threads. you need to serve this html file with these two headers: https://web.dev/coop-coep/
        ort.env.wasm.numThreads = navigator.hardwareConcurrency / 2;
      }
      
      ort.logLevel = "verbose";
      ort.logLevelInternal = "verbose";

      let sessionOptions = {
        executionProviders: [ backend ],
        graphOptimizationLevel: 'all',
      };
	    
      if(new URL(onnxModelUrl).pathname.endsWith(".ort")) {
        // See here for details: github.com/microsoft/onnxruntime/issues/13445#issuecomment-1430153341
        sessionOptions = {
          executionProviders: [ backend ],
          enableMemPattern: false,
          enableCpuMemArena: false,
          extra: {
            session: {
              disable_prepacking: "1",
              use_device_allocator_for_initializers: "0",
              use_ort_model_bytes_directly: "1",
              use_ort_model_bytes_for_initializers: "1",
            },
          },
        };
      }

      let onnxModelBlob = await downloadBlobWithProgress(onnxModelUrl, function(e) {
        let ratio = e.loaded / e.total;
        downloadProgressEl.value = ratio;
        loadingStatusEl.innerHTML = "Downloading..." + Math.round(ratio*e.total/1e6)+" MB";
      });

      loadingStatusEl.innerHTML = "Initializing...";
      
      let onnxModelBlobUrl = URL.createObjectURL(onnxModelBlob);

      session = await ort.InferenceSession.create(onnxModelBlobUrl, sessionOptions);
      
      URL.revokeObjectURL(onnxModelBlobUrl);
      
    } catch (e) {
      console.log(`Failed to load ONNX model: ${e}.`);
      console.error(e);
    }
    
    async function predictText(promptText, numTokensToGenerate=32, streamingCallback=null) {

      let startTime = Date.now();

      const xx_att_d = new Float32Array(n_layer*n_embd);
      const aa_att_d = new Float32Array(n_layer*n_embd);
      const bb_att_d = new Float32Array(n_layer*n_embd);
      const pp_att_d = new Float32Array(n_layer*n_embd);
      const xx_ffn_d = new Float32Array(n_layer*n_embd);

      pp_att_d.fill(-1e30);

      const xx_att = new ort.Tensor('float32', xx_att_d, [n_layer, n_embd]);
      const aa_att = new ort.Tensor('float32', aa_att_d, [n_layer, n_embd]);
      const bb_att = new ort.Tensor('float32', bb_att_d, [n_layer, n_embd]);
      const pp_att = new ort.Tensor('float32', pp_att_d, [n_layer, n_embd]);
      const xx_ffn = new ort.Tensor('float32', xx_ffn_d, [n_layer, n_embd]);

      // prepare feeds. use model input names as keys.
      let feeds = { idx: null, xx_att: xx_att, aa_att: aa_att, bb_att: bb_att, pp_att: pp_att, xx_ffn: xx_ffn };

      let promptTokens = textToTokens(promptText);
      let ctx = [ promptTokens.shift() ];

      // feed inputs and run
      for (let i = 0; i < numTokensToGenerate; i++) {
        let idx_d = Int32Array.from( padLeftWithZeros(ctx, 1024) );
        let idx = new ort.Tensor('int32', idx_d, [1024]);

        feeds.idx = idx;

        let results = await session.run(feeds);
        let token = greedySampling(results.x.data);

        if (promptTokens.length == 0) {
          if(streamingCallback) streamingCallback(token);
          ctx.push( token );
        } else {
          ctx.push( promptTokens.shift() );
        }

        feeds.xx_att = results.xx_att_r;
        feeds.aa_att = results.aa_att_r;
        feeds.bb_att = results.bb_att_r;
        feeds.pp_att = results.pp_att_r;
        feeds.xx_ffn = results.xx_ffn_r;

        progressDisplayEl.innerHTML = `Progress: ${i+1}/${numTokensToGenerate}`;
      }
      
      let timeTaken = Date.now() - startTime;
      console.log(`Finished. Took ${timeTaken}ms`);
      tokensPerSecDisplayEl.innerHTML = `(${(numTokensToGenerate/(timeTaken/1000)).toFixed(2)} tokens/sec)`;

      return tokensToText(ctx);
    }
    
    inputAreaEl.style.display = "";
    loadingMessageEl.style.display = "none";
    
    generateButtonEl.onclick = async function() {
      generateButtonEl.disabled = true;
      generateButtonEl.innerHTML = "Loading...";
      
      tokensPerSecDisplayEl.innerHTMl = "";
	    
      let promptText = promptInputEl.value;
      let numTokensToGenerate = Number(numTokensToGenInputEl.value);
      
      async function streamingCallback(token) {
        let text = tokensToText([token]);
        promptInputEl.value += text;
      }
      
      let result = await predictText(promptText, numTokensToGenerate, streamingCallback);
      
      console.log(result);
			
      generateButtonEl.disabled = false;
      generateButtonEl.innerHTML = "generate";
    };</script>
