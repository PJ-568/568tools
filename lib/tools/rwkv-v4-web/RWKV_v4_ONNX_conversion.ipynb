{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RWKV-v4 ONNX conversion.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7i3tCWQ41NH"
      },
      "outputs": [],
      "source": [
        "!git clone --branch onnx https://github.com/AXKuhta/RWKV-LM\n",
        "%cd /content/RWKV-LM\n",
        "!git checkout 648bf3c81af1355343b9f36ee7dba57775c81ead # <-- to ensure this notebook works even if repo is updated"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/RWKV-LM/RWKV-v4"
      ],
      "metadata": {
        "id": "UCWs7ppT5Nj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "I83Bu96U5UUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/BlinkDL/rwkv-4-pile-169m/resolve/main/RWKV-4-Pile-169M-20220807-8023.pth\n",
        "MODEL_NAME = 'RWKV-4-Pile-169M-20220807-8023'\n",
        "n_layer = 12\n",
        "n_embd = 768\n",
        "ctx_len = 1024\n",
        "\n",
        "# !wget https://huggingface.co/BlinkDL/rwkv-4-pile-430m/resolve/main/RWKV-4-Pile-430M-20220808-8066.pth\n",
        "# MODEL_NAME = 'RWKV-4-Pile-430M-20220808-8066'\n",
        "# n_layer = 24\n",
        "# n_embd = 1024\n",
        "# ctx_len = 1024\n",
        "\n",
        "# !wget https://huggingface.co/BlinkDL/rwkv-4-pile-1b5/resolve/main/RWKV-4-Pile-1B5-20220814-4526.pth\n",
        "# MODEL_NAME = 'RWKV-4-Pile-1B5-20220814-4526'\n",
        "# n_layer = 24\n",
        "# n_embd = 2048\n",
        "# ctx_len = 1024"
      ],
      "metadata": {
        "id": "HSGyx4_Cs8js"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "os.environ['RWKV_FLOAT_MODE'] = 'fp32'\n",
        "os.environ['RWKV_RUN_DEVICE'] = 'cpu'\n",
        "model_type = 'RWKV'\n",
        "\n",
        "from src.model_run import RWKV_RNN\n",
        "\n",
        "np.set_printoptions(precision=4, suppress=True, linewidth=200)\n",
        "\n",
        "tokenizer = PreTrainedTokenizerFast(tokenizer_file='20B_tokenizer.json')\n",
        "context = '\\nIn a shocking finding,'\n",
        "\n",
        "model = RWKV_RNN(MODEL_NAME, os.environ['RWKV_RUN_DEVICE'], model_type, n_layer, n_embd, ctx_len)\n",
        "\n",
        "ctx = torch.randint(5000, (1024,), dtype=torch.int32 ) + 100\n",
        "xx_att = torch.zeros(n_layer, n_embd)\n",
        "aa_att = torch.zeros(n_layer, n_embd)\n",
        "bb_att = torch.zeros(n_layer, n_embd)\n",
        "pp_att = torch.zeros(n_layer, n_embd) - 1e30\n",
        "xx_ffn = torch.zeros(n_layer, n_embd)\n",
        "\n",
        "torch.onnx.export(model, args=(ctx, xx_att, aa_att, bb_att, pp_att, xx_ffn), f=\"rwkv.onnx\", input_names = [\"idx\", \"xx_att\", \"aa_att\", \"bb_att\", \"pp_att\", \"xx_ffn\"], output_names = [\"x\", \"xx_att_r\", \"aa_att_r\", \"bb_att_r\", \"pp_att_r\", \"xx_ffn_r\"], verbose=True)"
      ],
      "metadata": {
        "id": "oHAfyUjQvFLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we quantize the model:\n",
        "!pip install onnxruntime\n",
        "!pip install onnx\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType"
      ],
      "metadata": {
        "id": "GpvwFCAb-Y7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantize_dynamic(\"/content/RWKV-LM/RWKV-v4/rwkv.onnx\", \"/content/RWKV-LM/RWKV-v4/rwkv-uint8.onnx\", weight_type=QuantType.QUInt8, extra_options={\"MatMulConstBOnly\":False})"
      ],
      "metadata": {
        "id": "xot0pp9a-VVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ORT format is useful because it needs half the memory of ONNX models during init: https://github.com/microsoft/onnxruntime/issues/13445#issuecomment-1430153341\n",
        "!python -m onnxruntime.tools.convert_onnx_models_to_ort \"/content/RWKV-LM/RWKV-v4/rwkv.onnx\""
      ],
      "metadata": {
        "id": "jaB63R5jAE5k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
